1 - When we pre-process the training examples, why are we adding a column of ones to the left of the x vector
(or X matrix) when we use the linear algebra trick?
to be able to manage theta0, so we can directly dot product x and theta

2 - Why does the cost function square the distance between the data points and their predicted values?
So it doesnt nullify neg values with pos ones, and we also got stronger cost for stronger errors, which is nice

3 - What does the cost function value represent?
Te cost to use the predictive model, istead of true values

4 - Toward which value would you like the cost function to tend to? What would it mean?
We want it to be 0, which will mean it's the same as the real values (relatively to the cost fonction)
